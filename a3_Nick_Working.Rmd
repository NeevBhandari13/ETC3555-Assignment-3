
```{r}
#file.choose()
SEED <- 1
```

```{r, message=FALSE}
library(tidyverse)
library(dplyr)
library(ggplot2)
library(caret)
library(glmnet)
library(reticulate)
library(keras3)
#keras::install_keras(tensorflow = "2.17")
```

library(reticulate)
virtualenv_install(envname = "r-reticulate", packages = c("keras", "tensorflow", "numpy"))
library(keras)
library(tensorflow)

# Check if tensorflow is working
tf$constant("Hello TensorFlow!")
py_config()





####  Predictor Variables (X)

The predictor variables used in this analysis represent a wide range of patient characteristics and clinical measurements, including:

**Vital Signs:** These include minimum, maximum, and mean values of heart rate, systolic blood pressure (SysBP), diastolic blood pressure (DiasBP), respiratory rate, temperature, and oxygen saturation (SpO2). These are critical indicators of a patient's physiological condition during their ICU stay.

**Demographic Information:** Variables such as GENDER, DOB (date of birth), and ETHNICITY provide insight into the patient’s background.

**Admission Information:** Variables like ADMITTIME, ADMISSION_TYPE, and INSURANCE indicate the circumstances and context of the patient’s hospital admission.

**Diagnosis Information:** The DIAGNOSIS variable captures the primary diagnosis at the time of ICU admission, while ICD9_diagnosis provides the corresponding ICD-9 code.

##### Independent Variables: mimic_train_X.csv

**subject_id**: Unique identifier for each patient.
**hadm_id**: Hospital admission ID, unique for each hospital admission.
**icustay_id**: Unique identifier for each ICU stay.
**HeartRate_Min**: The minimum heart rate recorded during the ICU stay.
**HeartRate_Max**: The maximum heart rate recorded during the ICU stay.
**HeartRate_Mean**: The average heart rate during the ICU stay.
**SysBP_Min**: Minimum systolic blood pressure recorded during the ICU stay.
**SysBP_Max**: Maximum systolic blood pressure recorded during the ICU stay.
**SysBP_Mean**: The average systolic blood pressure during the ICU stay.
**DiasBP_Min**: Minimum diastolic blood pressure recorded during the ICU stay.
**DiasBP_Max**: Maximum diastolic blood pressure recorded during the ICU stay.
**DiasBP_Mean**: The average diastolic blood pressure during the ICU stay.
**MeanBP_Min**: Minimum mean arterial blood pressure recorded during the ICU stay.
**MeanBP_Max**: Maximum mean arterial blood pressure recorded during the ICU stay.
**MeanBP_Mean**: The average mean arterial blood pressure during the ICU stay.
**RespRate_Min**: Minimum respiratory rate recorded during the ICU stay.
**RespRate_Max**: Maximum respiratory rate recorded during the ICU stay.
**RespRate_Mean**: The average respiratory rate during the ICU stay.
**TempC_Min**: Minimum body temperature (in Celsius) recorded during the ICU stay.
**TempC_Max**: Maximum body temperature (in Celsius) recorded during the ICU stay.
**TempC_Mean**: The average body temperature (in Celsius) during the ICU stay.
**SpO2_Min**: Minimum oxygen saturation (SpO2) level recorded during the ICU stay.
**SpO2_Max**: Maximum oxygen saturation (SpO2) level recorded during the ICU stay.
**SpO2_Mean**: The average oxygen saturation (SpO2) level during the ICU stay.
**Glucose_Min**: Minimum blood glucose level recorded during the ICU stay.
**Glucose_Max**: Maximum blood glucose level recorded during the ICU stay.
**Glucose_Mean**: The average blood glucose level during the ICU stay.

**GENDER**: Gender of the patient.
**DOB**: Date of birth of the patient.
**ADMITTIME**: Timestamp of the patient's hospital admission.
**Diff**: Time difference between hospital admission and another event.
**ADMISSION_TYPE**: Type of admission (e.g., Emergency, Elective).
**INSURANCE**: Patient's insurance type.
**RELIGION**: Patient’s religion.
**MARITAL_STATUS**: Marital status of the patient.
**ETHNICITY**: Patient’s ethnicity.
**DIAGNOSIS**: Initial diagnosis at the time of ICU admission.
**ICD9_diagnosis**: The main diagnosis code in the ICD-9 system.
**FIRST_CAREUNIT**: The first ICU care unit the patient was admitted to (e.g., MICU, SICU, TSICU).


#### Target Variables (Y)

Two possible target variables (dependent variables) are of interest in this analysis:

**HOSPITAL_EXPIRE_FLAG:** A binary variable that indicates whether the patient passed away during their hospital stay (1 = expired, 0 = survived). This will be used for classification tasks.

**LOS (Length of Stay):** A continuous variable representing the total number of days a patient stayed in the hospital. This will be used for regression tasks aimed at predicting the duration of the patient’s stay.


##### Dependent Variables



```{r}
train_x = read.csv("mimic_data/mimic_train_x.csv")
train_y = read.csv("mimic_data/mimic_train_y.csv")
test_x = read.csv("mimic_data/mimic_test_x.csv")
test_y = read.csv("mimic_data/mimic_test_y.csv")
```

```{r}
train_y <- train_y %>% select(-X, -HOSPITAL_EXPIRE_FLAG)
test_y <- test_y %>% select(-X, -HOSPITAL_EXPIRE_FLAG)
```

```{r}

clean_df_lm <- function(df) {
  df$DOB <- as.Date(df$DOB)
  df$ADMITTIME <- as.POSIXct(df$ADMITTIME, format="%Y-%m-%d %H:%M:%S")

  df$AGE <- as.numeric(difftime(df$ADMITTIME, df$DOB, units = "days")) / 365.25

  df <- df %>% select(-c(DOB, ADMITTIME))

  categorical_cols <- c("GENDER", "ADMISSION_TYPE", "INSURANCE", "RELIGION", 
                      "MARITAL_STATUS", "ETHNICITY", "DIAGNOSIS", 
                      "ICD9_diagnosis", "FIRST_CAREUNIT")

  df <- df %>% mutate(across(all_of(categorical_cols), ~ as.numeric(as.factor(.))))
  
}



clean_df_nn <- function(df) {
  # Convert date columns to Date and POSIXct format
  df$DOB <- as.Date(df$DOB)
  df$ADMITTIME <- as.POSIXct(df$ADMITTIME, format="%Y-%m-%d %H:%M:%S")
  
  # Calculate the patient's age and remove date columns
  df$AGE <- as.numeric(difftime(df$ADMITTIME, df$DOB, units = "days")) / 365.25
  df <- df %>% select(-c(DOB, ADMITTIME))  # Drop the date columns
  
  # Use model.matrix for both numeric and categorical variables, no need to convert to factor manually
  df_numeric <- model.matrix(~ . - 1, data = df) %>%  # Create dummy variables and remove intercept
    as.data.frame()
  
  return(df_numeric)
}

# Step 1: Preprocess the training and test datasets for lm
train_df_lm <- clean_df_lm(merge(train_x, train_y, by = "icustay_id"))
test_df_lm <- clean_df_lm(merge(test_x, test_y, by = "icustay_id"))

# Step 1: Preprocess the training and test datasets for nn
train_df_nn <- clean_df_nn(merge(train_x, train_y, by = "icustay_id"))
test_df_nn <- clean_df_nn(merge(test_x, test_y, by = "icustay_id"))


# Clean the training and test datasets using clean_df
train_x <- train_df_nn %>% select(-LOS) # Remove target variable LOS
train_y <- train_df_nn %>% select(LOS)  # Target variable

test_x <- test_df_nn %>% select(-LOS)  # Remove target variable LOS
test_y <- test_df_nn %>% select(LOS)  # Target variable

train_x <- scale(train_x)
test_x <- scale(test_x)

dim(train_df)
dim(train_x)
dim(train_y)

```




```{r}
train_df_lm <- subset(train_df_lm, LOS >= 0 & LOS <= 70)
summary(train_df_lm$LOS)
```

```{r}
#summary(train_df)

ggplot(train_df_lm, aes(x = LOS)) + 
  geom_histogram(binwidth = .5, fill = "blue", color = "black") + 
  theme_minimal() +
  labs(title = "Distribution of Length of Stay", x = "Length of Stay (days)", y = "Count")

```

ggplot(train_df, aes(x = factor(HOSPITAL_EXPIRE_FLAG))) + 
  geom_bar(fill = "blue") + 
  theme_minimal() +
  labs(title = "Distribution of Hospital Expire Flag", x = "Hospital Expire Flag", y = "Count")




```{r}
plot(train_df_lm$LOS, train_df_lm$HeartRate_Mean,
     main = "Scatter Plot: LOS vs HeartRate_Mean",
     xlab = "Length of Stay (LOS) in Days",
     ylab = "Heart Rate Mean",
     col = "blue", pch = 19)

plot(train_df_lm$LOS, train_df_lm$SysBP_Mean,
     main = "Scatter Plot: LOS vs SysBP_Mean",
     xlab = "Length of Stay (LOS) in Days",
     ylab = "Systolic Blood Pressure Mean",
     col = "red", pch = 19)

plot(train_df_lm$LOS, train_df_lm$RespRate_Mean,
     main = "Scatter Plot: LOS vs RespRate_Mean",
     xlab = "Length of Stay (LOS) in Days",
     ylab = "Respiratory Rate Mean",
     col = "purple", pch = 19)

plot(train_df_lm$LOS, train_df_lm$TempC_Mean,
     main = "Scatter Plot: LOS vs TempC_Mean",
     xlab = "Length of Stay (LOS) in Days",
     ylab = "Temperature Mean (C)",
     col = "green", pch = 19)
```


boxplot(train_df_lm$HeartRate_Mean ~ train_df_lm$HOSPITAL_EXPIRE_FLAG,
        main = "Heart Rate Mean by Hospital Expire Flag",
        xlab = "Hospital Expire Flag (0 = Survived, 1 = Death)",
        ylab = "Heart Rate Mean",
        col = c("lightblue", "lightcoral"))

boxplot(train_df_lm$SysBP_Mean ~ train_df_lm$HOSPITAL_EXPIRE_FLAG,
        main = "Systolic Blood Pressure Mean by Hospital Expire Flag",
        xlab = "Hospital Expire Flag (0 = Survived, 1 = Expired)",
        ylab = "Systolic Blood Pressure Mean",
        col = c("lightblue", "lightcoral"))

boxplot(train_df_lm$TempC_Mean ~ train_df_lm$HOSPITAL_EXPIRE_FLAG,
        main = "Temperature Mean by Hospital Expire Flag",
        xlab = "Hospital Expire Flag (0 = Survived, 1 = Expired)",
        ylab = "Temperature Mean (C)",
        col = c("lightblue", "lightcoral"))

boxplot(train_df_lm$RespRate_Mean ~ train_df_lm$HOSPITAL_EXPIRE_FLAG,
        main = "Respiratory Rate Mean by Hospital Expire Flag",
        xlab = "Hospital Expire Flag (0 = Survived, 1 = Expired)",
        ylab = "Respiratory Rate Mean",
        col = c("lightblue", "lightcoral"))




Data Partition

set.seed(123)  
train_df_lm_train <- createDataPartition(train_df_lm$HOSPITAL_EXPIRE_FLAG, p = 0.8, list = FALSE)



model <- lm(LOS ~ HeartRate_Mean + SysBP_Mean + DiasBP_Mean + RespRate_Mean + TempC_Mean + SpO2_Mean + Glucose_Mean + GENDER + ADMISSION_TYPE + INSURANCE + RELIGION + MARITAL_STATUS + ETHNICITY + DIAGNOSIS + ICD9_diagnosis + FIRST_CAREUNIT, data = train_df_lm)

summary(model)

```{r}
model <- lm(LOS ~ HeartRate_Mean + SysBP_Mean + DiasBP_Mean + RespRate_Mean + TempC_Mean + SpO2_Mean + Glucose_Mean + GENDER + ADMISSION_TYPE + INSURANCE + RELIGION + MARITAL_STATUS + ETHNICITY + FIRST_CAREUNIT, data = train_df_lm)
summary(model)
```


Testing Linear Model



```{r}
predicted_los <- predict(model, newdata = test_df_lm)

# Add the predicted values to the test dataframe
test_df_lm$Predicted_LOS <- predicted_los

# View the results
head(test_df_lm[, c("LOS", "Predicted_LOS")])
```

```{r}
test_model <- function(model, test_df_lm, train_df_lm) {
  train_df_lm$Predicted_LOS <- predict(model, newdata = train_df_lm)
  
  in_sample_mse <- mean((train_df_lm$LOS - train_df_lm$Predicted_LOS)^2)
  
  test_df_lm$Predicted_LOS <- predict(model, newdata = test_df_lm)
  
  out_of_sample_mse <- mean((test_df_lm$LOS - test_df_lm$Predicted_LOS)^2)
  
  print(paste("In-sample error (MSE):", in_sample_mse))
  print(paste("Out-of-sample error (MSE):", out_of_sample_mse))
}

test_model(model, test_df_lm, train_df_lm)

```

Non-linear transforms

First, I will plot each numerical variable against length of stay to see if I can spot any patterns.

```{r}
predictors <- c("HeartRate_Mean", "SysBP_Mean", "DiasBP_Mean", "RespRate_Mean", "TempC_Mean", "SpO2_Mean", "Glucose_Mean")

# Generate individual scatter plots for mean values
for (variable in predictors) {
  # Group by the predictor and calculate the mean LOS
  mean_df <- train_df_lm %>%
    group_by_at(variable) %>%
    summarise(mean_LOS = mean(LOS, na.rm = TRUE)) %>%
    ungroup()

  # Plot the mean LOS for each unique value of the predictor
  p <- ggplot(mean_df, aes_string(x = variable, y = "mean_LOS")) +
    geom_point(alpha = 0.8, color = "blue") +
    theme_minimal() +
    labs(title = paste(variable, "Mean vs LOS"), x = variable, y = "Mean Length of Stay (LOS)") +
    geom_smooth(method = "loess", color = "red", se = FALSE)  

  print(p)
}

```


***non-linear transformation model

```{r}
model_non_linear <- lm(LOS ~ HeartRate_Mean + SysBP_Mean + DiasBP_Mean + RespRate_Mean + exp(TempC_Mean) + exp(SpO2_Mean) + log(Glucose_Mean) + GENDER + ADMISSION_TYPE + INSURANCE + RELIGION + MARITAL_STATUS + ETHNICITY + FIRST_CAREUNIT, data = train_df_lm)

summary(model_non_linear)

test_model(model_non_linear, test_df_lm, train_df_lm)
```


NEURAL NETWORK


```{r}

# Find missing columns in test_x that are present in train_x
missing_cols <- setdiff(colnames(train_x), colnames(test_x))

# Create a matrix of zeros with the same number of rows as test_x and as many columns as there are missing_cols
missing_matrix <- matrix(0, nrow = nrow(test_x), ncol = length(missing_cols))

# Set the column names of the new matrix to the missing column names
colnames(missing_matrix) <- missing_cols

# Bind the missing columns to test_x in one step
test_x <- cbind(test_x, missing_matrix)

# Ensure the columns in test_x are in the same order as in train_x
test_x <- test_x[, colnames(train_x)]


```

```{r}
# Convert train_x and train_y to matrix and numeric types
train_x <- as.matrix(train_x)
train_y <- as.matrix(train_y)

# Similarly, for test data
test_x <- as.matrix(test_x)
test_y <- as.matrix(test_y)
```

```{r}
nn <- keras_model_sequential()

nn %>% 
  # First hidden layer with 16 units, L2 regularization, and He initializer
  layer_dense(units = 50, 
              input_shape = ncol(train_x), 
              kernel_regularizer = regularizer_l2(0.01), 
              kernel_initializer = initializer_he_normal(seed = 1),  # He initializer
              bias_initializer = initializer_zeros()) %>%  # Initialize bias as zeros
  layer_activation_leaky_relu(alpha = 0.1) %>%  # Leaky ReLU with alpha = 0.1
  
  # Output layer with 1 unit (for regression), linear activation, and He initializer
  layer_dense(units = 1, activation = 'linear', 
              kernel_regularizer = regularizer_l2(0.01), 
              kernel_initializer = initializer_he_normal(seed = 1),  # He initializer for output
              bias_initializer = initializer_zeros())  # Initialize bias as zeros

# Display model summary
summary(nn)

```

```{r}
nn %>% compile(
  loss = 'mse',
  optimizer = 'adam'
)
```

```{r}
history <- nn %>% fit(
  train_x, train_y,
  epochs = 100, batch_size = 128,
  validation_split = 0.2, 
  callbacks = list(
    callback_early_stopping(monitor = "val_loss", patience = 5, restore_best_weights = TRUE)
  )
)
```

```{r}
predicted_y <- predict(nn, test_x)

mse <- mean((test_y - predicted_y)^2)

print(paste("Test MSE:", mse))
```


DEEP NEURAL NETWORK

```{r}
dnn <- keras_model_sequential()

# Adding L2 regularization, Dropout for regularization, and Leaky ReLU activation
dnn %>% 
  # First hidden layer with 128 units, L2 regularization, and He initializer
  layer_dense(units = 100, 
              input_shape = ncol(train_x), 
              kernel_regularizer = regularizer_l2(0.01), 
              kernel_initializer = initializer_he_normal(seed = 1),  # He initializer
              bias_initializer = initializer_zeros()) %>%
  layer_activation_leaky_relu(alpha = 0.1) %>%  # Leaky ReLU activation
  layer_dropout(rate = 0.2) %>%  # Dropout to reduce overfitting
  
  # Second hidden layer with 64 units, L2 regularization, and He initializer
  layer_dense(units = 50, 
              kernel_regularizer = regularizer_l2(0.01), 
              kernel_initializer = initializer_he_normal(seed = 1),  # He initializer
              bias_initializer = initializer_zeros()) %>%
  layer_activation_leaky_relu(alpha = 0.1) %>%  # Leaky ReLU activation
  layer_dropout(rate = 0.5) %>%  # Another Dropout layer
  
  # Third hidden layer with 32 units, L2 regularization, and He initializer
  layer_dense(units = 25, 
              kernel_regularizer = regularizer_l2(0.01), 
              kernel_initializer = initializer_he_normal(seed = 1),  # He initializer
              bias_initializer = initializer_zeros()) %>%
  layer_activation_leaky_relu(alpha = 0.1) %>%  # Leaky ReLU activation
  layer_dropout(rate = 0.5) %>%  # Another Dropout layer
  
  # Output layer with 1 unit (for regression), linear activation, and He initializer
  layer_dense(units = 1, activation = 'linear', 
              kernel_regularizer = regularizer_l2(0.01), 
              kernel_initializer = initializer_he_normal(seed = 1),  # He initializer
              bias_initializer = initializer_zeros())  # Initialize bias as zeros

# Display summary of the model
summary(dnn)


```

```{r}
dnn %>% compile(
  loss = 'mse',
  optimizer = 'adam'
)
```

```{r}
history <- dnn %>% fit(
  train_x, train_y,
  epochs = 100, batch_size = 50,
  validation_split = 0.2, 
  callbacks = list(
    callback_early_stopping(monitor = "val_loss", patience = 5, restore_best_weights = TRUE)
  )
)
```

```{r}
predicted_y <- predict(dnn, test_x)

mse <- mean((test_y - predicted_y)^2)

print(paste("Test MSE:", mse))
```

